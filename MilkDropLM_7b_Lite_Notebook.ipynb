{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "history_visible": true,
      "gpuType": "V28",
      "authorship_tag": "ABX9TyMFemikBgq4itSDuqTN++mn",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "TPU",
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "f62d2ad1a5e14c14bb8c438fac44a012": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_b2b77c7dab0546d8b0212e1baf392c5e",
              "IPY_MODEL_5914506fc7f84530a67fcfc10af26c4a",
              "IPY_MODEL_d1cd7a63527f44228e1c1bb001dbad25"
            ],
            "layout": "IPY_MODEL_16e887d08789457abf43a50e2e2bd3a3"
          }
        },
        "b2b77c7dab0546d8b0212e1baf392c5e": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_1c3d37b7f8aa4c5b8616748a4780c074",
            "placeholder": "​",
            "style": "IPY_MODEL_bac66527a3d84b638e8d743c5b629449",
            "value": "MilkDropLM-v0.3C-Q5_K_S.gguf: 100%"
          }
        },
        "5914506fc7f84530a67fcfc10af26c4a": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_c2906085f3b04336826e3b2b6c702816",
            "max": 5315176704,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_3309ac9c494c4b72be59c8a95d00cf57",
            "value": 5315176704
          }
        },
        "d1cd7a63527f44228e1c1bb001dbad25": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_61e5b55162ba429d8ab2f03ad069bf45",
            "placeholder": "​",
            "style": "IPY_MODEL_c709602a70d74a8691963a70b65502e8",
            "value": " 5.32G/5.32G [02:06&lt;00:00, 42.7MB/s]"
          }
        },
        "16e887d08789457abf43a50e2e2bd3a3": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "1c3d37b7f8aa4c5b8616748a4780c074": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "bac66527a3d84b638e8d743c5b629449": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "c2906085f3b04336826e3b2b6c702816": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "3309ac9c494c4b72be59c8a95d00cf57": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "61e5b55162ba429d8ab2f03ad069bf45": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "c709602a70d74a8691963a70b65502e8": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        }
      }
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/InferenceIllusionist/MilkDropLM-Lite/blob/main/MilkDropLM_7b_Lite_Notebook.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "xVV9GdBQG3-H"
      },
      "outputs": [],
      "source": [
        "# MilKDropLM Version: 7b-0.3 @ Q6_K\n",
        "# Lite (Notebook) Version: 1.1b - New Gradio front-end\n",
        "# To Run: Go to Runtime -> Run All, then find Gradio UI or URL at bottom of notebook\n",
        "\n",
        "# Install required packages\n",
        "%pip install https://github.com/abetlen/llama-cpp-python/releases/download/v0.3.4-cu124/llama_cpp_python-0.3.4-cp310-cp310-linux_x86_64.whl\n",
        "%pip install gradio\n",
        "\n",
        "import gradio as gr\n",
        "import torch\n",
        "from llama_cpp import Llama\n",
        "import time"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Initialize the model globally to avoid reloading\n",
        "llm = Llama.from_pretrained(\n",
        "    repo_id=\"Quant-Cartel/MilkDropLM-7b-v0.3-GGUF\",\n",
        "    filename=\"*Q6_K.gguf\",\n",
        "    n_ctx=16384,\n",
        "    n_gpu_layers=-1,\n",
        "    max_tokens=8192,\n",
        "    temperature=0.3\n",
        ")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000,
          "referenced_widgets": [
            "f62d2ad1a5e14c14bb8c438fac44a012",
            "b2b77c7dab0546d8b0212e1baf392c5e",
            "5914506fc7f84530a67fcfc10af26c4a",
            "d1cd7a63527f44228e1c1bb001dbad25",
            "16e887d08789457abf43a50e2e2bd3a3",
            "1c3d37b7f8aa4c5b8616748a4780c074",
            "bac66527a3d84b638e8d743c5b629449",
            "c2906085f3b04336826e3b2b6c702816",
            "3309ac9c494c4b72be59c8a95d00cf57",
            "61e5b55162ba429d8ab2f03ad069bf45",
            "c709602a70d74a8691963a70b65502e8"
          ]
        },
        "id": "FpysxzluMJYD",
        "outputId": "113768ae-ded6-446e-c161-84d60cbd2392"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/huggingface_hub/utils/_auth.py:94: UserWarning: \n",
            "The secret `HF_TOKEN` does not exist in your Colab secrets.\n",
            "To authenticate with the Hugging Face Hub, create a token in your settings tab (https://huggingface.co/settings/tokens), set it as secret in your Google Colab and restart your session.\n",
            "You will be able to reuse this secret in all of your notebooks.\n",
            "Please note that authentication is recommended but still optional to access public models or datasets.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "MilkDropLM-v0.3C-Q5_K_S.gguf:   0%|          | 0.00/5.32G [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "f62d2ad1a5e14c14bb8c438fac44a012"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "llama_model_loader: loaded meta data with 31 key-value pairs and 339 tensors from /root/.cache/huggingface/hub/models--Quant-Cartel--MilkDropLM-7b-v0.3-GGUF/snapshots/ed3d5a7e561418a3b4e86a6f73ff41db946a9893/./MilkDropLM-v0.3C-Q5_K_S.gguf (version GGUF V3 (latest))\n",
            "llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.\n",
            "llama_model_loader: - kv   0:                       general.architecture str              = qwen2\n",
            "llama_model_loader: - kv   1:                               general.type str              = model\n",
            "llama_model_loader: - kv   2:                               general.name str              = Qwen2.5 Coder 7B Instruct\n",
            "llama_model_loader: - kv   3:                       general.organization str              = Unsloth\n",
            "llama_model_loader: - kv   4:                           general.finetune str              = Instruct\n",
            "llama_model_loader: - kv   5:                           general.basename str              = Qwen2.5-Coder\n",
            "llama_model_loader: - kv   6:                         general.size_label str              = 7B\n",
            "llama_model_loader: - kv   7:                   general.base_model.count u32              = 1\n",
            "llama_model_loader: - kv   8:                  general.base_model.0.name str              = Qwen2.5 Coder 7B Instruct Bnb 4bit\n",
            "llama_model_loader: - kv   9:          general.base_model.0.organization str              = Unsloth\n",
            "llama_model_loader: - kv  10:              general.base_model.0.repo_url str              = https://huggingface.co/unsloth/Qwen2....\n",
            "llama_model_loader: - kv  11:                          qwen2.block_count u32              = 28\n",
            "llama_model_loader: - kv  12:                       qwen2.context_length u32              = 32768\n",
            "llama_model_loader: - kv  13:                     qwen2.embedding_length u32              = 3584\n",
            "llama_model_loader: - kv  14:                  qwen2.feed_forward_length u32              = 18944\n",
            "llama_model_loader: - kv  15:                 qwen2.attention.head_count u32              = 28\n",
            "llama_model_loader: - kv  16:              qwen2.attention.head_count_kv u32              = 4\n",
            "llama_model_loader: - kv  17:                       qwen2.rope.freq_base f32              = 1000000.000000\n",
            "llama_model_loader: - kv  18:     qwen2.attention.layer_norm_rms_epsilon f32              = 0.000001\n",
            "llama_model_loader: - kv  19:                          general.file_type u32              = 16\n",
            "llama_model_loader: - kv  20:                       tokenizer.ggml.model str              = gpt2\n",
            "llama_model_loader: - kv  21:                         tokenizer.ggml.pre str              = qwen2\n",
            "llama_model_loader: - kv  22:                      tokenizer.ggml.tokens arr[str,152064]  = [\"!\", \"\\\"\", \"#\", \"$\", \"%\", \"&\", \"'\", ...\n",
            "llama_model_loader: - kv  23:                  tokenizer.ggml.token_type arr[i32,152064]  = [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...\n",
            "llama_model_loader: - kv  24:                      tokenizer.ggml.merges arr[str,151387]  = [\"Ġ Ġ\", \"ĠĠ ĠĠ\", \"i n\", \"Ġ t\",...\n",
            "llama_model_loader: - kv  25:                tokenizer.ggml.eos_token_id u32              = 151645\n",
            "llama_model_loader: - kv  26:                tokenizer.ggml.bos_token_id u32              = 151643\n",
            "llama_model_loader: - kv  27:            tokenizer.ggml.padding_token_id u32              = 151665\n",
            "llama_model_loader: - kv  28:               tokenizer.ggml.add_bos_token bool             = false\n",
            "llama_model_loader: - kv  29:                    tokenizer.chat_template str              = {%- if tools %}\\n    {{- '<|im_start|>...\n",
            "llama_model_loader: - kv  30:               general.quantization_version u32              = 2\n",
            "llama_model_loader: - type  f32:  141 tensors\n",
            "llama_model_loader: - type q5_K:  197 tensors\n",
            "llama_model_loader: - type q6_K:    1 tensors\n",
            "llm_load_vocab: special tokens cache size = 23\n",
            "llm_load_vocab: token to piece cache size = 0.9310 MB\n",
            "llm_load_print_meta: format           = GGUF V3 (latest)\n",
            "llm_load_print_meta: arch             = qwen2\n",
            "llm_load_print_meta: vocab type       = BPE\n",
            "llm_load_print_meta: n_vocab          = 152064\n",
            "llm_load_print_meta: n_merges         = 151387\n",
            "llm_load_print_meta: vocab_only       = 0\n",
            "llm_load_print_meta: n_ctx_train      = 32768\n",
            "llm_load_print_meta: n_embd           = 3584\n",
            "llm_load_print_meta: n_layer          = 28\n",
            "llm_load_print_meta: n_head           = 28\n",
            "llm_load_print_meta: n_head_kv        = 4\n",
            "llm_load_print_meta: n_rot            = 128\n",
            "llm_load_print_meta: n_swa            = 0\n",
            "llm_load_print_meta: n_embd_head_k    = 128\n",
            "llm_load_print_meta: n_embd_head_v    = 128\n",
            "llm_load_print_meta: n_gqa            = 7\n",
            "llm_load_print_meta: n_embd_k_gqa     = 512\n",
            "llm_load_print_meta: n_embd_v_gqa     = 512\n",
            "llm_load_print_meta: f_norm_eps       = 0.0e+00\n",
            "llm_load_print_meta: f_norm_rms_eps   = 1.0e-06\n",
            "llm_load_print_meta: f_clamp_kqv      = 0.0e+00\n",
            "llm_load_print_meta: f_max_alibi_bias = 0.0e+00\n",
            "llm_load_print_meta: f_logit_scale    = 0.0e+00\n",
            "llm_load_print_meta: n_ff             = 18944\n",
            "llm_load_print_meta: n_expert         = 0\n",
            "llm_load_print_meta: n_expert_used    = 0\n",
            "llm_load_print_meta: causal attn      = 1\n",
            "llm_load_print_meta: pooling type     = 0\n",
            "llm_load_print_meta: rope type        = 2\n",
            "llm_load_print_meta: rope scaling     = linear\n",
            "llm_load_print_meta: freq_base_train  = 1000000.0\n",
            "llm_load_print_meta: freq_scale_train = 1\n",
            "llm_load_print_meta: n_ctx_orig_yarn  = 32768\n",
            "llm_load_print_meta: rope_finetuned   = unknown\n",
            "llm_load_print_meta: ssm_d_conv       = 0\n",
            "llm_load_print_meta: ssm_d_inner      = 0\n",
            "llm_load_print_meta: ssm_d_state      = 0\n",
            "llm_load_print_meta: ssm_dt_rank      = 0\n",
            "llm_load_print_meta: ssm_dt_b_c_rms   = 0\n",
            "llm_load_print_meta: model type       = ?B\n",
            "llm_load_print_meta: model ftype      = Q5_K - Small\n",
            "llm_load_print_meta: model params     = 7.62 B\n",
            "llm_load_print_meta: model size       = 4.94 GiB (5.58 BPW) \n",
            "llm_load_print_meta: general.name     = Qwen2.5 Coder 7B Instruct\n",
            "llm_load_print_meta: BOS token        = 151643 '<|endoftext|>'\n",
            "llm_load_print_meta: EOS token        = 151645 '<|im_end|>'\n",
            "llm_load_print_meta: PAD token        = 151665 '<|PAD_TOKEN|>'\n",
            "llm_load_print_meta: LF token         = 148848 'ÄĬ'\n",
            "llm_load_print_meta: EOT token        = 151645 '<|im_end|>'\n",
            "llm_load_print_meta: max token length = 256\n",
            "ggml_cuda_init: GGML_CUDA_FORCE_MMQ:    yes\n",
            "ggml_cuda_init: GGML_CUDA_FORCE_CUBLAS: no\n",
            "ggml_cuda_init: found 1 CUDA devices:\n",
            "  Device 0: Tesla T4, compute capability 7.5, VMM: yes\n",
            "llm_load_tensors: ggml ctx size =    0.30 MiB\n",
            "llm_load_tensors: offloading 28 repeating layers to GPU\n",
            "llm_load_tensors: offloading non-repeating layers to GPU\n",
            "llm_load_tensors: offloaded 29/29 layers to GPU\n",
            "llm_load_tensors:        CPU buffer size =   357.33 MiB\n",
            "llm_load_tensors:      CUDA0 buffer size =  4705.94 MiB\n",
            ".......................................................................................\n",
            "llama_new_context_with_model: n_ctx      = 16384\n",
            "llama_new_context_with_model: n_batch    = 512\n",
            "llama_new_context_with_model: n_ubatch   = 512\n",
            "llama_new_context_with_model: flash_attn = 0\n",
            "llama_new_context_with_model: freq_base  = 1000000.0\n",
            "llama_new_context_with_model: freq_scale = 1\n",
            "llama_kv_cache_init:      CUDA0 KV buffer size =   896.00 MiB\n",
            "llama_new_context_with_model: KV self size  =  896.00 MiB, K (f16):  448.00 MiB, V (f16):  448.00 MiB\n",
            "llama_new_context_with_model:  CUDA_Host  output buffer size =     0.58 MiB\n",
            "llama_new_context_with_model:      CUDA0 compute buffer size =   956.00 MiB\n",
            "llama_new_context_with_model:  CUDA_Host compute buffer size =    39.01 MiB\n",
            "llama_new_context_with_model: graph nodes  = 986\n",
            "llama_new_context_with_model: graph splits = 2\n",
            "AVX = 1 | AVX_VNNI = 0 | AVX2 = 1 | AVX512 = 0 | AVX512_VBMI = 0 | AVX512_VNNI = 0 | AVX512_BF16 = 0 | FMA = 1 | NEON = 0 | SVE = 0 | ARM_FMA = 0 | F16C = 1 | FP16_VA = 0 | WASM_SIMD = 0 | BLAS = 1 | SSE3 = 1 | SSSE3 = 1 | VSX = 0 | MATMUL_INT8 = 0 | LLAMAFILE = 1 | \n",
            "Model metadata: {'tokenizer.ggml.bos_token_id': '151643', 'general.file_type': '16', 'tokenizer.ggml.eos_token_id': '151645', 'qwen2.rope.freq_base': '1000000.000000', 'tokenizer.chat_template': '{%- if tools %}\\n    {{- \\'<|im_start|>system\\\\n\\' }}\\n    {%- if messages[0][\\'role\\'] == \\'system\\' %}\\n        {{- messages[0][\\'content\\'] }}\\n    {%- else %}\\n        {{- \\'You are Qwen, created by Alibaba Cloud. You are a helpful assistant.\\' }}\\n    {%- endif %}\\n    {{- \"\\\\n\\\\n# Tools\\\\n\\\\nYou may call one or more functions to assist with the user query.\\\\n\\\\nYou are provided with function signatures within <tools></tools> XML tags:\\\\n<tools>\" }}\\n    {%- for tool in tools %}\\n        {{- \"\\\\n\" }}\\n        {{- tool | tojson }}\\n    {%- endfor %}\\n    {{- \"\\\\n</tools>\\\\n\\\\nFor each function call, return a json object with function name and arguments within <tool_call></tool_call> XML tags:\\\\n<tool_call>\\\\n{\\\\\"name\\\\\": <function-name>, \\\\\"arguments\\\\\": <args-json-object>}\\\\n</tool_call><|im_end|>\\\\n\" }}\\n{%- else %}\\n    {%- if messages[0][\\'role\\'] == \\'system\\' %}\\n        {{- \\'<|im_start|>system\\\\n\\' + messages[0][\\'content\\'] + \\'<|im_end|>\\\\n\\' }}\\n    {%- else %}\\n        {{- \\'<|im_start|>system\\\\nYou are Qwen, created by Alibaba Cloud. You are a helpful assistant.<|im_end|>\\\\n\\' }}\\n    {%- endif %}\\n{%- endif %}\\n{%- for message in messages %}\\n    {%- if (message.role == \"user\") or (message.role == \"system\" and not loop.first) or (message.role == \"assistant\" and not message.tool_calls) %}\\n        {{- \\'<|im_start|>\\' + message.role + \\'\\\\n\\' + message.content + \\'<|im_end|>\\' + \\'\\\\n\\' }}\\n    {%- elif message.role == \"assistant\" %}\\n        {{- \\'<|im_start|>\\' + message.role }}\\n        {%- if message.content %}\\n            {{- \\'\\\\n\\' + message.content }}\\n        {%- endif %}\\n        {%- for tool_call in message.tool_calls %}\\n            {%- if tool_call.function is defined %}\\n                {%- set tool_call = tool_call.function %}\\n            {%- endif %}\\n            {{- \\'\\\\n<tool_call>\\\\n{\"name\": \"\\' }}\\n            {{- tool_call.name }}\\n            {{- \\'\", \"arguments\": \\' }}\\n            {{- tool_call.arguments | tojson }}\\n            {{- \\'}\\\\n</tool_call>\\' }}\\n        {%- endfor %}\\n        {{- \\'<|im_end|>\\\\n\\' }}\\n    {%- elif message.role == \"tool\" %}\\n        {%- if (loop.index0 == 0) or (messages[loop.index0 - 1].role != \"tool\") %}            {{- \\'<|im_start|>user\\' }}\\n        {%- endif %}\\n        {{- \\'\\\\n<tool_response>\\\\n\\' }}\\n        {{- message.content }}\\n        {{- \\'\\\\n</tool_response>\\' }}\\n        {%- if loop.last or (messages[loop.index0 + 1].role != \"tool\") %}\\n            {{- \\'<|im_end|>\\\\n\\' }}\\n        {%- endif %}\\n    {%- endif %}\\n{%- endfor %}\\n{%- if add_generation_prompt %}\\n    {{- \\'<|im_start|>assistant\\\\n\\' }}\\n{%- endif %}\\n', 'qwen2.attention.head_count_kv': '4', 'qwen2.attention.head_count': '28', 'general.quantization_version': '2', 'tokenizer.ggml.model': 'gpt2', 'qwen2.feed_forward_length': '18944', 'general.architecture': 'qwen2', 'tokenizer.ggml.padding_token_id': '151665', 'qwen2.embedding_length': '3584', 'general.basename': 'Qwen2.5-Coder', 'tokenizer.ggml.add_bos_token': 'false', 'general.base_model.0.organization': 'Unsloth', 'tokenizer.ggml.pre': 'qwen2', 'general.name': 'Qwen2.5 Coder 7B Instruct', 'general.base_model.0.name': 'Qwen2.5 Coder 7B Instruct Bnb 4bit', 'qwen2.block_count': '28', 'qwen2.attention.layer_norm_rms_epsilon': '0.000001', 'general.organization': 'Unsloth', 'general.base_model.0.repo_url': 'https://huggingface.co/unsloth/Qwen2.5-Coder-7B-Instruct-bnb-4bit', 'general.finetune': 'Instruct', 'general.type': 'model', 'general.size_label': '7B', 'general.base_model.count': '1', 'qwen2.context_length': '32768'}\n",
            "Available chat formats from metadata: chat_template.default\n",
            "Using gguf chat template: {%- if tools %}\n",
            "    {{- '<|im_start|>system\\n' }}\n",
            "    {%- if messages[0]['role'] == 'system' %}\n",
            "        {{- messages[0]['content'] }}\n",
            "    {%- else %}\n",
            "        {{- 'You are Qwen, created by Alibaba Cloud. You are a helpful assistant.' }}\n",
            "    {%- endif %}\n",
            "    {{- \"\\n\\n# Tools\\n\\nYou may call one or more functions to assist with the user query.\\n\\nYou are provided with function signatures within <tools></tools> XML tags:\\n<tools>\" }}\n",
            "    {%- for tool in tools %}\n",
            "        {{- \"\\n\" }}\n",
            "        {{- tool | tojson }}\n",
            "    {%- endfor %}\n",
            "    {{- \"\\n</tools>\\n\\nFor each function call, return a json object with function name and arguments within <tool_call></tool_call> XML tags:\\n<tool_call>\\n{\\\"name\\\": <function-name>, \\\"arguments\\\": <args-json-object>}\\n</tool_call><|im_end|>\\n\" }}\n",
            "{%- else %}\n",
            "    {%- if messages[0]['role'] == 'system' %}\n",
            "        {{- '<|im_start|>system\\n' + messages[0]['content'] + '<|im_end|>\\n' }}\n",
            "    {%- else %}\n",
            "        {{- '<|im_start|>system\\nYou are Qwen, created by Alibaba Cloud. You are a helpful assistant.<|im_end|>\\n' }}\n",
            "    {%- endif %}\n",
            "{%- endif %}\n",
            "{%- for message in messages %}\n",
            "    {%- if (message.role == \"user\") or (message.role == \"system\" and not loop.first) or (message.role == \"assistant\" and not message.tool_calls) %}\n",
            "        {{- '<|im_start|>' + message.role + '\\n' + message.content + '<|im_end|>' + '\\n' }}\n",
            "    {%- elif message.role == \"assistant\" %}\n",
            "        {{- '<|im_start|>' + message.role }}\n",
            "        {%- if message.content %}\n",
            "            {{- '\\n' + message.content }}\n",
            "        {%- endif %}\n",
            "        {%- for tool_call in message.tool_calls %}\n",
            "            {%- if tool_call.function is defined %}\n",
            "                {%- set tool_call = tool_call.function %}\n",
            "            {%- endif %}\n",
            "            {{- '\\n<tool_call>\\n{\"name\": \"' }}\n",
            "            {{- tool_call.name }}\n",
            "            {{- '\", \"arguments\": ' }}\n",
            "            {{- tool_call.arguments | tojson }}\n",
            "            {{- '}\\n</tool_call>' }}\n",
            "        {%- endfor %}\n",
            "        {{- '<|im_end|>\\n' }}\n",
            "    {%- elif message.role == \"tool\" %}\n",
            "        {%- if (loop.index0 == 0) or (messages[loop.index0 - 1].role != \"tool\") %}            {{- '<|im_start|>user' }}\n",
            "        {%- endif %}\n",
            "        {{- '\\n<tool_response>\\n' }}\n",
            "        {{- message.content }}\n",
            "        {{- '\\n</tool_response>' }}\n",
            "        {%- if loop.last or (messages[loop.index0 + 1].role != \"tool\") %}\n",
            "            {{- '<|im_end|>\\n' }}\n",
            "        {%- endif %}\n",
            "    {%- endif %}\n",
            "{%- endfor %}\n",
            "{%- if add_generation_prompt %}\n",
            "    {{- '<|im_start|>assistant\\n' }}\n",
            "{%- endif %}\n",
            "\n",
            "Using chat eos_token: <|im_end|>\n",
            "Using chat bos_token: <|endoftext|>\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Instantiate Generation with Samplers\n",
        "def generate_preset(\n",
        "    prompt,\n",
        "    temperature=0.3,\n",
        "    top_p=0.8,\n",
        "    top_k=20,\n",
        "    min_p=0.05,\n",
        "    repeat_penalty=1.05,\n",
        "    max_tokens=8192\n",
        "):\n",
        "    try:\n",
        "        # Add a default prefix if user doesn't provide one\n",
        "        if not \"preset\" in prompt.lower():\n",
        "            prompt = f\"Make me a {prompt} milkdrop preset.\"\n",
        "\n",
        "        # Start time for generation\n",
        "        start_time = time.time()\n",
        "\n",
        "        # Generate response\n",
        "        response = llm(\n",
        "            prompt,\n",
        "            max_tokens=max_tokens,\n",
        "            temperature=temperature,\n",
        "            top_p=top_p,\n",
        "            top_k=top_k,\n",
        "            min_p=min_p,\n",
        "            repeat_penalty=repeat_penalty,\n",
        "\n",
        "        )\n",
        "\n",
        "        # Calculate generation time\n",
        "        generation_time = time.time() - start_time\n",
        "\n",
        "        # Format the response\n",
        "        formatted_response = f\"\"\"\n",
        "Generation Time: {generation_time:.2f} seconds\n",
        "\n",
        "Generated Preset:\n",
        "{response['choices'][0]['text']}\n",
        "\"\"\"\n",
        "        return formatted_response\n",
        "\n",
        "    except Exception as e:\n",
        "        return f\"An error occurred: {str(e)}\"\n"
      ],
      "metadata": {
        "id": "E2lmbYl0MS-Q"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Create the Gradio interface\n",
        "with gr.Blocks(theme=gr.themes.Soft()) as demo:\n",
        "    gr.Markdown(\"\"\"\n",
        "    # 🎵 MilkDropLM Preset Generator Lite\n",
        "    Generate custom Milkdrop presets using natural language prompts.\n",
        "    \"\"\")\n",
        "\n",
        "    with gr.Row():\n",
        "        with gr.Column():\n",
        "            prompt = gr.Textbox(\n",
        "                label=\"Your Preset Description\",\n",
        "                placeholder=\"e.g., Glowsticks Mirror\",\n",
        "                lines=3\n",
        "            )\n",
        "\n",
        "            with gr.Accordion(\"Advanced Settings\", open=False):\n",
        "                temperature = gr.Slider(\n",
        "                    minimum=0.1,\n",
        "                    maximum=1.0,\n",
        "                    value=0.3,\n",
        "                    step=0.1,\n",
        "                    label=\"Temperature\"\n",
        "                )\n",
        "                top_p = gr.Slider(\n",
        "                    minimum=0.1,\n",
        "                    maximum=1.0,\n",
        "                    value=0.8,\n",
        "                    step=0.1,\n",
        "                    label=\"Top P\"\n",
        "                )\n",
        "                top_k = gr.Slider(\n",
        "                    minimum=1,\n",
        "                    maximum=100,\n",
        "                    value=20,\n",
        "                    step=1,\n",
        "                    label=\"Top K\"\n",
        "                )\n",
        "                min_p = gr.Slider(\n",
        "                    minimum=0.01,\n",
        "                    maximum=0.2,\n",
        "                    value=0.05,\n",
        "                    step=0.01,\n",
        "                    label=\"Min P\"\n",
        "                )\n",
        "                repeat_penalty = gr.Slider(\n",
        "                    minimum=1.0,\n",
        "                    maximum=2.0,\n",
        "                    value=1.05,\n",
        "                    step=0.05,\n",
        "                    label=\"Repeat Penalty\"\n",
        "                )\n",
        "\n",
        "            generate_btn = gr.Button(\"Generate Preset\", variant=\"primary\")\n",
        "\n",
        "        with gr.Column():\n",
        "            output = gr.Textbox(\n",
        "                label=\"Generated Preset\",\n",
        "                lines=20,\n",
        "                show_copy_button=True\n",
        "            )\n",
        "\n",
        "    # Handle the generation\n",
        "    generate_btn.click(\n",
        "        fn=generate_preset,\n",
        "        inputs=[\n",
        "            prompt,\n",
        "            temperature,\n",
        "            top_p,\n",
        "            top_k,\n",
        "            min_p,\n",
        "            repeat_penalty\n",
        "        ],\n",
        "        outputs=output\n",
        "    )\n",
        "\n",
        "    gr.Markdown(\"\"\"\n",
        "    ### Tips for Better Results:\n",
        "    - Please be patient, generation may take up to 5-6 minutes per preset on a free colab T4\n",
        "    - Try to prompt for categories suggested on the [model card](https://huggingface.co/Quant-Cartel/MilkDropLM-7b-v0.3-GGUF#text-prompt-suggestions)\n",
        "    - Mention colors, shapes, and movement patterns\n",
        "    \"\"\")"
      ],
      "metadata": {
        "id": "Y_Uy4GNbMtnX"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "demo.launch(share=True)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 715
        },
        "id": "Cmjinr3pM-FS",
        "outputId": "20e43d9f-3836-461d-d688-26e1489ab3e5"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Colab notebook detected. This cell will run indefinitely so that you can see errors and logs. To turn off, set debug=False in launch().\n",
            "* Running on public URL: https://15924ae5a3695ef11b.gradio.live\n",
            "\n",
            "This share link expires in 72 hours. For free permanent hosting and GPU upgrades, run `gradio deploy` from the terminal in the working directory to deploy to Hugging Face Spaces (https://huggingface.co/spaces)\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "<div><iframe src=\"https://15924ae5a3695ef11b.gradio.live\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n",
            "llama_print_timings:        load time =     537.65 ms\n",
            "llama_print_timings:      sample time =   13591.43 ms /  5815 runs   (    2.34 ms per token,   427.84 tokens per second)\n",
            "llama_print_timings: prompt eval time =     537.49 ms /     6 tokens (   89.58 ms per token,    11.16 tokens per second)\n",
            "llama_print_timings:        eval time =  190115.75 ms /  5814 runs   (   32.70 ms per token,    30.58 tokens per second)\n",
            "llama_print_timings:       total time =  249583.26 ms /  5820 tokens\n"
          ]
        }
      ]
    }
  ]
}